#! /usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved


"""
Storms SQS images queue with messages which will be inferred as S3 uploads
by the pdq_hasher lambda. Given a bucket and a prefix, gets files in that
bucket, for each creates SQS messages and rotates through that list until it
has published enough messages.

If the bucket with the prefix has more than 1000 keys, only the first 1000
keys returned are used.

$ ./storm_images_queue <queue_url> <s3_bucket> <prefix> <count>
Example
$ ./storm_images_queue https://queue.amazonaws.com/<acid>/<queue_name> <bucket_name> images/10MB/ 100000
"""

import sys
import argparse
import boto3
import json


SQS_BATCH_SIZE = 10 # matches the batch size in s3 subscription

def unleash_storm(
    queue_url: str,
    s3_bucket: str,
    s3_prefix: str,
    msg_count: int
):
    s3_client = boto3.client('s3')

    # Obtain the images
    response = s3_client.list_objects_v2(
        Bucket=s3_bucket,
        Prefix=s3_prefix,
    )

    images = list(map(lambda x: x['Key'], response['Contents']))
    image_chunks = [images[i:i + SQS_BATCH_SIZE] for i in range(0, len(images), SQS_BATCH_SIZE)]

    sqs_client = boto3.client('sqs')

    sent_batch_count = 0
    sent_message_count = 0
    while sent_message_count < msg_count:
        current_chunk = image_chunks[sent_batch_count % len(image_chunks)]

        # Process one chunk begin
        sqs_client.send_message_batch(
            QueueUrl=queue_url,
            Entries=[{
                        'Id': f'message-{sent_batch_count}-{i}',
                        'MessageBody':  json.dumps({
                            'Message': json.dumps({
                                'Records': [{
                                    's3': {
                                        'bucket': {
                                            'name': s3_bucket,
                                        },
                                        'object': {
                                            'key': image,
                                            'size': 12 # Any positive number works.
                                        }
                                    }
                                }]
                            })
                        })
                    } for i, image in enumerate(current_chunk)]
        )
        # Process one chunk ends

        sent_message_count += len(current_chunk)
        sent_batch_count += 1

    print("Sent", sent_message_count, "messages in", sent_batch_count, "batches")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Storm images queue with fake upload events. Will trigger lambdas and some cost $$"
    )
    parser.add_argument(
        "queue_url",
        type=str,
        help="SQS Queue URL. Must include account id. eg. https://queue.amazonaws.com/<acid>/<queue_name>",
    )

    parser.add_argument(
        "s3_bucket",
        type=str,
        help="S3 Bucket Name. Only provide the name, protocol and slashes are not required."
    )

    parser.add_argument(
        "s3_prefix",
        type=str,
        help="S3 Bucket prefix for images you want to use. Only use a trailing slash. eg. images/100KB"
    )

    parser.add_argument(
        "count",
        type=int,
        help="How many messages to send? Note this will be approximate, not exact."
    )

    args = parser.parse_args()

    unleash_storm(
        args.queue_url,
        args.s3_bucket,
        args.s3_prefix,
        args.count
    )
