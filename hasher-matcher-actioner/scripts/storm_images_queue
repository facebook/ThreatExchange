#! /usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved


"""
Storms SQS images queue with messages which will be inferred as S3 uploads
by the pdq_hasher lambda. Given a bucket and a prefix, gets files in that
bucket, for each creates SQS messages and rotates through that list until it
has published enough messages.

If the bucket with the prefix has more than 1000 keys, only the first 1000
keys returned are used.

$ ./storm_images_queue <queue_url> <s3_bucket> <prefix> <count>
Example
$ ./storm_images_queue https://queue.amazonaws.com/<acid>/<queue_name> <bucket_name> images/10MB/ 100000
"""

import sys
import typing as t
import argparse
import boto3
import json
import concurrent.futures


SQS_BATCH_SIZE = 10 # matches the batch size in s3 subscription

def _send_sqs_message(
    queue_url: str,
    s3_bucket: str,
    current_chunk: t.List[str],
    sent_batch_count: int,
):
    sqs_client = boto3.client('sqs')
    sqs_client.send_message_batch(
        QueueUrl=queue_url,
        Entries=[
            {
                "Id": f"message-{sent_batch_count}-{i}",
                "MessageBody": json.dumps(
                    {
                        "Message": json.dumps(
                            {
                                "Records": [
                                    {
                                        "s3": {
                                            "bucket": {
                                                "name": s3_bucket,
                                            },
                                            "object": {
                                                "key": image,
                                                "size": 12,  # Any positive number works.
                                            },
                                        }
                                    }
                                ]
                            }
                        )
                    }
                ),
            }
            for i, image in enumerate(current_chunk)
        ],
    )


def unleash_storm(
    queue_url: str,
    s3_bucket: str,
    s3_prefix: str,
    msg_count: int
):
    s3_client = boto3.client('s3')

    # Obtain the images
    response = s3_client.list_objects_v2(
        Bucket=s3_bucket,
        Prefix=s3_prefix,
    )

    images = list(map(lambda x: x['Key'], response['Contents']))
    image_chunks = [images[i:i + SQS_BATCH_SIZE] for i in range(0, len(images), SQS_BATCH_SIZE)]

    sent_batch_count = 0
    sent_message_count = 0
    jobs = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
        while sent_message_count < msg_count:
            current_chunk = image_chunks[sent_batch_count % len(image_chunks)]

            jobs.append(executor.submit(_send_sqs_message, queue_url, s3_bucket, current_chunk, sent_batch_count))

            # executors are fire-and-forget, so we increment counters as
            # soon as the message is dropped.
            sent_message_count += len(current_chunk)
            sent_batch_count += 1

        for i, completed_future in enumerate(concurrent.futures.as_completed(jobs)):
            # Report progress
            print(i, "of", sent_batch_count, "completed.", end='\r')

    print("Sent", sent_message_count, "messages in", sent_batch_count, "batches")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Storm images queue with fake upload events. Will trigger lambdas and some cost $$"
    )
    parser.add_argument(
        "queue_url",
        help="SQS Queue URL. Must include account id. eg. https://queue.amazonaws.com/<acid>/<queue_name>",
    )

    parser.add_argument(
        "s3_bucket",
        help="S3 Bucket Name. Only provide the name, protocol and slashes are not required."
    )

    parser.add_argument(
        "s3_prefix",
        help="S3 Bucket prefix for images you want to use. Only use a trailing slash. eg. images/100KB"
    )

    parser.add_argument(
        "count",
        type=int,
        help="How many messages to send? Note this will be approximate, not exact."
    )

    args = parser.parse_args()

    unleash_storm(
        args.queue_url,
        args.s3_bucket,
        args.s3_prefix,
        args.count
    )
