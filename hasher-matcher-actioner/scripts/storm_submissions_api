#! /usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved

"""
Storms the submissions API for a running HMA instance with images.

Try to resist the urge to add any hmalib dependencies here. This script can be
run on a server which does not have hmalib.

Sample Usage:
```
$ ./storm_submissions_api https://m....f.execute-api.us-east-1.amazonaws.com/ ../copydays-extended/200100.jpg 100
```
"""

import os
import base64
import argparse
import typing as t
import json
import uuid
import datetime
import requests
import urllib3
import concurrent.futures
import functools
import socket
from time import perf_counter
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

from hmalib.scripts.common.utils import HasherMatcherActionerAPI

# Token can be found using Postman or with developer credentials using script/get_auth_token
TOKEN = ""

# "/" does not work with react router preventing the content submission details from rendering.
# However it can be used here for easier clean up between storms that do not need the UI.
ID_SEPARATOR = "-"

# hostnames are especially useful when storming from more than one instance.
hostname = socket.gethostname()

# Value to incorperate into content_id as an attempt to get greater prefix optimizations (ymmv).
# bucket_prefix_known = int(socket.gethostname()[-2:]) % 5


def _submit_via_upload_put_url(
    api: HasherMatcherActionerAPI, filepath: str, additional_fields: t.List[str]
) -> int:
    """
    Submit a single file by requesting a presigned url and return the time it took in ms.
    """
    file_name = os.path.split(filepath)[-1]
    content_id = f"storm_url-{hostname}{ID_SEPARATOR}{datetime.date.today().isoformat()}{ID_SEPARATOR}{str(uuid.uuid4())}-{file_name}"

    start_time = perf_counter()

    with open(filepath, "rb") as file:
        try:
            api.submit_via_upload_put_url(
                content_id,
                file,
                additional_fields,
            )  # api method calls raise_for_status to check for error
        except (
            urllib3.exceptions.MaxRetryError,
            requests.exceptions.HTTPError,
            requests.exceptions.ConnectionError,
            requests.exceptions.Timeout,
            requests.exceptions.RequestException,
        ) as err:
            print("Error:", err)

    # convert seconds to miliseconds.
    return int((perf_counter() - start_time) * 1000)


@functools.lru_cache(maxsize=None)
def _get_b64_contents(filepath):
    with open(filepath, "rb") as file:
        return str(base64.b64encode(file.read()), "utf-8")


def _submit_via_encoded_bytes(
    api: HasherMatcherActionerAPI, filepath: str, additional_fields: t.List[str]
) -> int:
    """
    Submit a single file and return the time it took in ms.
    """
    file_name = os.path.split(filepath)[-1]
    content_id = f"storm-{hostname}{ID_SEPARATOR}{datetime.date.today().isoformat()}{ID_SEPARATOR}{str(uuid.uuid4())}-{file_name}"

    start_time = perf_counter()

    try:
        api.submit_via_encoded_bytes(
            content_id, _get_b64_contents(filepath), additional_fields
        )  # api method calls raise_for_status to check for error
    except (
        urllib3.exceptions.MaxRetryError,
        requests.exceptions.HTTPError,
        requests.exceptions.ConnectionError,
        requests.exceptions.Timeout,
        requests.exceptions.RequestException,
    ) as err:
        print("Error:", err)

    # convert seconds to miliseconds.
    return int((perf_counter() - start_time) * 1000)


def unleash_storm(
    api_url: str,
    filepath: str,
    msg_count: int,
    token: str,
    url_mode: bool,
    verbose: bool,
    retry: bool,
):
    sent_message_count = 0
    jobs = []

    execution_times = []

    send_single_submission_func = _submit_via_encoded_bytes
    if url_mode:
        send_single_submission_func = _submit_via_upload_put_url

    if retry:
        retry_strategy = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=[
                "GET",
                "PUT",
                "POST",
            ],  # Including POST as the API should not perform an insert if an error is returned
        )
        api = HasherMatcherActionerAPI(
            api_url, token, transport_adapter=HTTPAdapter(max_retries=retry_strategy)
        )
    else:
        api = HasherMatcherActionerAPI(api_url, token)

    # Need to compute script level RequestsPerSecond so that we can estimate
    # benchmark performance. For that, storing the start time every 200
    # requests and reporting the QPS between that and current.

    with concurrent.futures.ThreadPoolExecutor(max_workers=48) as executor:
        if verbose:
            print("Adding tasks to executor")
        chunk_start_time_200 = perf_counter()

        while sent_message_count < msg_count:
            jobs.append(
                executor.submit(
                    send_single_submission_func,
                    api,
                    filepath,
                    [],
                )
            )

            sent_message_count += 1
            if verbose:
                print(f"{sent_message_count} requests prepared", end="\r")

        if verbose:
            print("Done")

        for i, completed_future in enumerate(concurrent.futures.as_completed(jobs)):
            execution_times.append(completed_future.result())
            if verbose:
                progress_report_string = f"{i} of {msg_count} sent!"

                rps_report_string = f"Current 200 chunk has QPS of {(i % 200) / (perf_counter() - chunk_start_time_200)}"

                # Report progress and RPS
                print(f"{progress_report_string}! {rps_report_string}", end="\r")

            if i % 200 == 0:
                # Reset chunk start time
                chunk_start_time_200 = perf_counter()

    if verbose:
        print(f"Sent all {msg_count} submissions.")

        # Compute some beginner stats.
        execution_times = sorted(execution_times)
        print(
            f"""Percentiles in ms:
                p75: {execution_times[int(len(execution_times)*0.75)]}
                p95: {execution_times[int(len(execution_times)*0.95)]}
                p99: {execution_times[int(len(execution_times)*0.99)]}
            """
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Storm the submisisons API with photo uploads. Will trigger lambdas and cost some $$"
    )
    parser.add_argument(
        "api_url",
        help="HMA API URL. Can be obtained by using terraform outputs. Look for 'api_url'",
    )
    parser.add_argument("file", help="The photo to upload.")
    parser.add_argument(
        "count",
        type=int,
        help="Approximately how many times do we want to send this photo?",
    )
    parser.add_argument(
        "--token",
        help="token required to access the API",
        default=TOKEN,
    )
    parser.add_argument(
        "--url_mode",
        action="store_true",
        help="Submit using presign putObject urls instead of base64 encoding json",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print progress and percentile times for submission sets.",
    )
    parser.add_argument(
        "--retry",
        action="store_true",
        help="Have request retry (with exponential backoff) if errors are encoutered.",
    )

    args = parser.parse_args()
    unleash_storm(
        args.api_url,
        args.file,
        args.count,
        args.token,
        args.url_mode,
        args.verbose,
        args.retry,
    )
